{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: From .pt File to ONNXRuntime Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.Convert `.pt` to `.onnx`\n",
    "\n",
    "### Model Import with Ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "working_dir = Path()\n",
    "model_weight = working_dir / 'models/yolov10n.pt'\n",
    "\n",
    "print(model_weight.resolve())\n",
    "\n",
    "model = YOLO(model_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export as `.onnx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(format=\"onnx\", dynamic=False, batch=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.Inference Samples\n",
    "\n",
    "### Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_SHAPE: [6, 3, 640, 640]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_weight = working_dir / 'models/yolov10x.onnx'\n",
    "EP_LIST = [\n",
    "    ('CUDAExecutionProvider', {'device_id': 0}), # To utilze GPU, onnxruntime-gpu is required\n",
    "    'CPUExecutionProvider',\n",
    "]\n",
    "\n",
    "# Create an inference session using the ONNX model and specify execution providers\n",
    "session = ort.InferenceSession(onnx_weight, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Get the model inputs\n",
    "model_inputs = session.get_inputs()\n",
    "\n",
    "# Store the shape of the input for later use\n",
    "input_shape = model_inputs[0].shape\n",
    "input_width = input_shape[2]\n",
    "input_height = input_shape[3]\n",
    "\n",
    "print(f\"INPUT_SHAPE: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "vid_path = working_dir / 'samples/sample1.mp4'\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "ret, im = cap.read()\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = (640,640)\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = im.shape[:2]\n",
    "\n",
    "NUM_SLICES = (3,2) # horizontal, width\n",
    "SLICE_LENGTH = int(IMAGE_HEIGHT * 11 / 18)\n",
    "RATIO_INPUT2SLICE = float(SLICE_LENGTH/INPUT_SZ[0])\n",
    "OFFSETS = (-int(SLICE_LENGTH / 22), -int(SLICE_LENGTH * 4 / 11)) # horizontal, width\n",
    "\n",
    "# Calculate slice start positions\n",
    "SLICE_POSITIONS = []\n",
    "for i in range(NUM_SLICES[1]):\n",
    "    for j in range(NUM_SLICES[0]):\n",
    "        x = j * (SLICE_LENGTH+OFFSETS[0])\n",
    "        y = i * (SLICE_LENGTH+OFFSETS[1])\n",
    "        SLICE_POSITIONS.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def slice_image(\n",
    "    image: np.ndarray,\n",
    "    output_file_name: Optional[str] = None,\n",
    "    output_dir: Optional[str] = None\n",
    ") -> Tuple[List[np.ndarray], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Slice a large image (numpy array) into a fixed number of windows: 3 horizontal slices and 2 vertical slices.\n",
    "    Each slice size is determined as 11/18 of the image's height.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image to be sliced as a numpy array (height x width x channels).\n",
    "        output_file_name (str, optional): Root name of output files.\n",
    "        output_dir (str, optional): Output directory.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A list of sliced images as numpy arrays and the directory of the exported images if applicable.\n",
    "            sliced_images[idx]: slice at (idx//NUM_SLICES[1], idx%NUM_SLICES[1])-th order\n",
    "    \"\"\"\n",
    "\n",
    "    sliced_images = []\n",
    "    for (x, y) in SLICE_POSITIONS:\n",
    "        slice_image = image[y:y + SLICE_LENGTH, x:x + SLICE_LENGTH]\n",
    "        sliced_images.append(slice_image)\n",
    "\n",
    "        # Save image if output_dir and output_file_name are provided\n",
    "        if output_file_name and output_dir:\n",
    "            if not Path(output_dir).exists():\n",
    "                Path(output_dir).mkdir(parents=True)\n",
    "            slice_file_name = f\"{output_file_name}_{x}_{y}.png\"\n",
    "            slice_file_path = Path(output_dir) / slice_file_name\n",
    "            Image.fromarray(slice_image).save(slice_file_path)\n",
    "\n",
    "    return sliced_images, output_dir if output_file_name and output_dir else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(im: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses the input image before performing inference.\n",
    "\n",
    "    Returns:\n",
    "        image_data: Preprocessed image data ready for inference.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image color space from BGR to RGB\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    slices, _ = slice_image(im)\n",
    "\n",
    "    # Resize the image to match the input shape\n",
    "    # Normalize the image data by dividing it by 255.0\n",
    "    slices_resized = np.array([cv2.resize(slc, INPUT_SZ, interpolation=cv2.INTER_LINEAR) for slc in slices]) / 255.0\n",
    "\n",
    "    # Transpose the image to have the channel dimension as the first dimension\n",
    "    image_data = np.transpose(slices_resized, (0, 3, 1, 2)).astype(np.float32)  # Batch, Channel, H, W\n",
    "\n",
    "    # Return the preprocessed image data\n",
    "    return np.ascontiguousarray(image_data)\n",
    "\n",
    "# Preprocess the image data\n",
    "im_input = preprocess(im)\n",
    "print(im_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inname = [i.name for i in session.get_inputs()]\n",
    "inname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output0']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outname = [i.name for i in session.get_outputs()]\n",
    "outname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[     15.056,    -0.94189,      631.69,      157.59,   0.0078498,           5],\n",
      "        [     539.25,      230.53,      639.83,      523.82,    0.006619,           2],\n",
      "        [     128.57,      372.87,      608.01,      536.34,   0.0056322,          10],\n",
      "        ...,\n",
      "        [     584.71,    -0.68367,      640.27,      109.94,  3.5822e-05,           1],\n",
      "        [    0.35823,      120.02,        43.7,       309.2,  3.5793e-05,           5],\n",
      "        [     487.37,      1.6805,      638.92,      188.28,  3.5703e-05,           7]],\n",
      "\n",
      "       [[     93.698,      111.04,      526.99,      510.33,    0.034215,           2],\n",
      "        [    -1.9896,    -0.70264,      271.52,      163.56,    0.012508,           5],\n",
      "        [     337.09,      586.71,      373.96,      625.69,   0.0054928,          10],\n",
      "        ...,\n",
      "        [     598.41,     0.17943,      643.93,      11.817,  2.9445e-05,           5],\n",
      "        [   0.034458,      256.91,       80.25,      449.54,  2.9117e-05,           5],\n",
      "        [  -0.015975,      146.58,      9.9707,      213.49,  2.8759e-05,          10]],\n",
      "\n",
      "       [[     369.08,      572.91,       455.4,      611.52,    0.027481,          10],\n",
      "        [     79.546,      68.947,      111.16,      99.246,    0.011812,          10],\n",
      "        [     623.99,      538.07,      639.93,      592.82,   0.0037858,          10],\n",
      "        ...,\n",
      "        [   0.081024,      520.96,      159.11,      639.39,  2.1458e-05,           7],\n",
      "        [     591.15,    0.098267,      635.62,      31.388,  2.1368e-05,           7],\n",
      "        [   -0.22939,      519.34,      11.576,      642.38,  2.1279e-05,           2]],\n",
      "\n",
      "       [[     1.3251,      331.77,       516.3,      615.11,     0.25654,           7],\n",
      "        [     295.13,    -0.58303,      579.49,      116.54,     0.13988,          10],\n",
      "        [     294.75,     -0.2073,      579.59,      110.99,    0.058339,          10],\n",
      "        ...,\n",
      "        [     614.41,      218.77,      639.87,      510.35,  1.1683e-05,           9],\n",
      "        [   -0.24376,      240.61,      239.92,      606.66,  1.1683e-05,           7],\n",
      "        [    -1.2014,    -0.20129,      83.569,      64.553,  1.1653e-05,          10]],\n",
      "\n",
      "       [[     108.16,      91.884,       317.5,      253.58,    0.061866,           3],\n",
      "        [     102.69,      19.096,      555.75,      287.69,    0.028268,          10],\n",
      "        [      2.281,      309.63,       639.1,      632.61,    0.011738,           9],\n",
      "        ...,\n",
      "        [     325.97,      292.95,      638.69,      638.22,   1.815e-05,           7],\n",
      "        [     562.12,    0.091881,      599.75,      7.9221,   1.803e-05,          10],\n",
      "        [     110.05,      90.874,      257.63,      164.75,  1.7941e-05,           6]],\n",
      "\n",
      "       [[     207.63,      214.33,      252.91,      259.67,    0.033136,          10],\n",
      "        [     369.36,      166.25,       455.9,      205.64,    0.026118,          10],\n",
      "        [    0.25314,       359.2,      412.07,       638.5,   0.0097986,           7],\n",
      "        ...,\n",
      "        [     10.464,     0.12494,      79.782,      10.895,  1.7077e-05,          10],\n",
      "        [    0.41489,      287.24,       94.25,      619.65,  1.6958e-05,           7],\n",
      "        [     338.39,      27.572,      352.28,      44.836,  1.6868e-05,          10]]], dtype=float32)]\n",
      "(6, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "# Run inference using the preprocessed image data\n",
    "outputs = session.run(None, {'images': im_input})\n",
    "\n",
    "print(outputs)\n",
    "print(outputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "classes: List[str] = [\n",
    "    'black_smoke',\n",
    "    'gray_smoke',\n",
    "    'white_smoke',\n",
    "    'flame',\n",
    "    'cloud',\n",
    "    'fog',\n",
    "    'lamp_light',\n",
    "    'sun_light',\n",
    "    'shaky_object',\n",
    "    'wind-swayed_leaves',\n",
    "    'irrelevant',\n",
    "]\n",
    "\n",
    "classes_aligned: List[Optional[str]] = [\n",
    "    'Smoke','Flame',None\n",
    "]\n",
    "\n",
    "classes_map: List[int] = [\n",
    "    0,0,0,1,2,2,2,2,2,2,2\n",
    "] # 0:smoke, 1:flame, 2:none\n",
    "\n",
    "color_palette: List[tuple] = [ # BGR\n",
    "    (255,0,0), # Smoke\n",
    "    (0,0,255), # Flame\n",
    "    (0,0,0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "- Reference: `Ultralytics/examples/YOLOv8-ONNXRuntime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_height, im_width = im.shape[:2]\n",
    "\n",
    "def draw_detections(img:np.ndarray, box, score, class_id) -> None:\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and labels on the input image based on the detected objects.\n",
    "\n",
    "    Args:\n",
    "        img: The input image to draw detections on.\n",
    "        box: Detected bounding box.\n",
    "        score: Corresponding detection score.\n",
    "        class_id: Class ID for the detected object.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if class_name := classes_aligned[class_id]:\n",
    "        pass\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Extract the coordinates of the bounding box\n",
    "    x1, y1, w, h = box\n",
    "\n",
    "    # Retrieve the color for the class ID\n",
    "    color = color_palette[class_id]\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(img, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "\n",
    "    # Create the label text with class name and score\n",
    "    label = f\"{class_name}: {score:.2f}\"\n",
    "\n",
    "    # Calculate the dimensions of the label text\n",
    "    (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "\n",
    "    # Calculate the position of the label text\n",
    "    label_x = x1\n",
    "    label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10\n",
    "\n",
    "    # Draw a filled rectangle as the background for the label text\n",
    "    cv2.rectangle(\n",
    "        img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED\n",
    "    )\n",
    "\n",
    "    # Draw the label text on the image\n",
    "    cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "def postprocess(input_image:np.ndarray, outputs:List[np.ndarray], confidence_thres:float=0.05, iou_thres:float=0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs post-processing on the model's output to extract bounding boxes, scores, and class IDs.\n",
    "\n",
    "    Args:\n",
    "        input_image (numpy.ndarray): The input image.\n",
    "        outputs[0] (numpy.ndarray): The outputs of the model.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The input image with detections drawn on it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists to store the bounding boxes, scores, and class IDs of the detections\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "\n",
    "    for outputs_slice, pos_slice in zip(outputs[0], SLICE_POSITIONS):\n",
    "        # Get the number of rows in the outputs array\n",
    "        rows = outputs_slice.shape[0]\n",
    "        \n",
    "        # Iterate over each row in the outputs array\n",
    "        for i in range(rows):\n",
    "            # Extract the score from the current row\n",
    "            best_score = outputs_slice[i][4]\n",
    "\n",
    "            # If the maximum score is above the confidence threshold\n",
    "            if best_score >= confidence_thres:\n",
    "                # Get the class ID with the highest score\n",
    "                class_id = int(outputs_slice[i][-1]) # detected class\n",
    "                class_id_aligned = classes_map[class_id]\n",
    "                \n",
    "                # Neglact `None` class\n",
    "                if classes_aligned[class_id_aligned]:\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Extract the bounding box coordinates from the current row\n",
    "                x1, y1, x2, y2 = outputs_slice[i][0], outputs_slice[i][1], outputs_slice[i][2], outputs_slice[i][3]\n",
    "\n",
    "                # Calculate the scaled coordinates of the bounding box\n",
    "                left = int(y1 * RATIO_INPUT2SLICE) + pos_slice[0]\n",
    "                top = int(x1* RATIO_INPUT2SLICE) + pos_slice[1]\n",
    "                width = int((y2-y1) * RATIO_INPUT2SLICE)\n",
    "                height = int((x2-x1) * RATIO_INPUT2SLICE)\n",
    "\n",
    "                # Add the class ID, score, and box coordinates to the respective lists\n",
    "                class_ids.append(class_id_aligned)\n",
    "                scores.append(best_score)\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    print(class_ids)\n",
    "    # Apply non-maximum suppression to filter out overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, scores, confidence_thres, iou_thres)\n",
    "    print(class_ids)\n",
    "\n",
    "    # Iterate over the selected indices after non-maximum suppression\n",
    "    for i in indices:\n",
    "        # Get the box, score, and class ID corresponding to the index\n",
    "        box = boxes[i]\n",
    "        score = scores[i]\n",
    "        class_id = class_ids[i]\n",
    "\n",
    "        # Draw the detection on the input image\n",
    "        draw_detections(input_image, box, score, class_id)\n",
    "\n",
    "    # Return the modified input image\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "final_output = postprocess(im, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('sample_result.jpg', final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
